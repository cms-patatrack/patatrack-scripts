#! /usr/bin/env python3
"""benchmark: run N jobs with T threads and S streams over N GPUs
"""
import argparse
import sys
import os
import copy

from multirun import *
import FWCore.ParameterSet.Config as cms

if __name__ == "__main__":
  if not 'CMSSW_BASE' in os.environ:
    print('Error: the CMS environment is not set up, please run "cmsenv" or "eval `scram runtime -sh`".')
    sys.exit(1)

  ### args
  parser = argparse.ArgumentParser(
   prog = './'+os.path.basename(__file__),
   formatter_class = argparse.RawDescriptionHelpFormatter,
   description = __doc__)

  parser.add_argument('configs',
                      type = str,
                      nargs = '+',
                      help = 'list of cmsRun configuration files')

  parser.add_argument('-v', '--verbose',
                      dest = 'verbose',
                      action = 'store_true',
                      default = False,
                      help = 'enable verbose mode [default: False]')

  parser.add_argument('-E', '--executable',
                      dest = 'executable',
                      action = 'store',
                      type = str,
                      default = 'cmsRun',
                      help = 'specify what executable to run [default: cmsRun]')

  parser.add_argument('-e', '--events',
                      dest = 'events',
                      action = 'store',
                      type = int,
                      default = 10300,
                      help = 'number of events per cmsRun job [default: 10300]')

  parser.add_argument('-j', '--jobs',
                      dest = 'jobs',
                      action = 'store',
                      type = int,
                      default = 2,
                      help = 'number of concurrent cmsRun jobs per measurement [default: 2]')

  parser.add_argument('-r', '--repeats',
                      dest = 'repeats',
                      action = 'store',
                      type = int,
                      default = 3,
                      help = 'repeat each measurement N times [default: 3]')

  parser.add_argument('-t', '--threads',
                      dest = 'threads',
                      action = 'store',
                      type = int,
                      default = None,
                      help = 'number of threads used in each cmsRun job [default: None -> set automatically to use the whole machine]')

  parser.add_argument('-s', '--streams',
                      dest = 'streams',
                      action = 'store',
                      type = int,
                      default = None,
                      help = 'number of streams used in each cmsRun job [default: None -> set automatically to use the whole machine]')

  parser.add_argument('-g', '--gpus-per-job',
                      dest = 'gpus_per_job',
                      action = 'store',
                      type = int,
                      default = 1,
                      help = 'number of GPUs used in each cmsRun job [default: 1]')

  group = parser.add_mutually_exclusive_group()
  parser.add_argument('--run-io-benchmark',
                      dest = 'run_io_benchmark',
                      action= 'store_true',
                      default = True,
                      help = 'measure the I/O benchmarks before the other measurements [default: True]')
  parser.add_argument('--no-run-io-benchmark',
                      dest = 'run_io_benchmark',
                      action= 'store_false',
                      help = 'skip the I/O measurement')

  group = parser.add_mutually_exclusive_group()
  group.add_argument('--warmup',
                     dest = 'warmup',
                     action = 'store_true',
                     default = True,
                     help = 'do a warmup run before the measurements [default: True]')
  group.add_argument('--no-warmup',
                     dest = 'warmup',
                     action = 'store_false',
                     help = 'skip the warmup run')

  group = parser.add_mutually_exclusive_group()
  group.add_argument('-p', '--plumbing',
                     dest = 'plumbing',
                     action = 'store_true',
                     default = False,
                     help = 'enable plumbing output [default: False]')
  group.add_argument('--no-plumbing',
                     dest = 'plumbing',
                     action = 'store_false',
                     help = 'disable plumbing output')

  group = parser.add_mutually_exclusive_group()
  group.add_argument('--allow-hyperthreading',
                     dest = 'allow_hyperthreading',
                     action = 'store_true',
                     default = True,
                     help = 'allow HyperThreading/Simultaneous multithreading (used only if cpu_affinity = True) [default: True]')
  group.add_argument('--no-hyperthreading',
                     dest = 'allow_hyperthreading',
                     action = 'store_false',
                     help = 'do not allow HyperThreading/Simultaneous multithreading (used only if cpu_affinity = True)')

  group = parser.add_mutually_exclusive_group()
  group.add_argument('-n', '--numa-affinity',
                     dest = 'numa_affinity',
                     action = 'store_true',
                     default = False,
                     help = 'enable NUMA affinity [default: False]')
  group.add_argument('--no-numa-affinity',
                     dest = 'numa_affinity',
                     action = 'store_false',
                     help = 'disable NUMA affinity')

  group = parser.add_mutually_exclusive_group()
  group.add_argument('--cpu-affinity',
                     dest = 'cpu_affinity',
                     action = 'store_true',
                     default = True,
                     help = 'enable CPU affinity [default: True]')
  group.add_argument('--no-cpu-affinity',
                     dest = 'cpu_affinity',
                     action = 'store_false',
                     help = 'disable CPU affinity')

  group = parser.add_mutually_exclusive_group()
  group.add_argument('--gpu-affinity',
                     dest = 'gpu_affinity',
                     action = 'store_true',
                     default = True,
                     help = 'enable GPU affinity [default: True]')
  group.add_argument('--no-gpu-affinity',
                     dest = 'gpu_affinity',
                     action = 'store_false',
                     help = 'disable GPU affinity')

  group = parser.add_mutually_exclusive_group()
  group.add_argument('-l', '--logdir',
                     dest = 'logdir',
                     action = 'store',
                     default = 'logs',
                     help = 'path to output directory for log files (if empty, logs are not stored) [default: "logs"]')
  group.add_argument('--no-logdir',
                     dest = 'logdir',
                     action = 'store_const',
                     const = 'None',
                     help = 'do not store log files (equivalent to "--logdir \'\'")')

  parser.add_argument('-k', '--keep',
                      dest = 'keep',
                      nargs = '+',
                      default = ['resources.json'],
                      help = 'list of additional output files to be kept in logdir, along with the logs [default: "resources.json"]')

  parser.add_argument('--tmpdir',
                      dest = 'tmpdir',
                      action = 'store',
                      default = None,
                      help = 'path to temporary directory used at runtime [default: None, to use a system-dependent default temporary directory]')

  opts, opts_unknown = parser.parse_known_args()
  ### ----

  if len(opts_unknown) > 0:
    raise RuntimeError('unsupported command-line arguments: '+str(opts_unknown))

  options = {
    'verbose'             : opts.verbose,
    'plumbing'            : opts.plumbing,
    'warmup'              : opts.warmup,
    'events'              : opts.events,
    'repeats'             : opts.repeats,
    'jobs'                : opts.jobs,
    'threads'             : opts.threads,
    'streams'             : opts.streams,
    'gpus_per_job'        : opts.gpus_per_job,
    'allow_hyperthreading': opts.allow_hyperthreading,
    'set_numa_affinity'   : opts.numa_affinity,
    'set_cpu_affinity'    : opts.cpu_affinity,
    'set_gpu_affinity'    : opts.gpu_affinity,
    'executable'          : opts.executable,
    'logdir'              : opts.logdir if opts.logdir else None,
    'tmpdir'              : opts.tmpdir,
    'keep'                : opts.keep,
  }

  # measure the throughput for reading the input data
  run_io_benchmark = opts.run_io_benchmark

  # print a system overview
  info()

  # check the available cpus
  cpus = get_cpu_info()
  if options['allow_hyperthreading']:
    count = sum(len(cpu.hardware_threads) for cpu in cpus.values())
  else:
    count = sum(len(cpu.physical_processors) for cpu in cpus.values())

  # autodetermine either the number of jobs ot the nuber of threads per job
  if options['threads'] is None and options['jobs'] is None:
    sys.stderr.write('%s: error: either the number of jobs ot the nuber of threads per job must be specified\n' % sys.argv[0])
  elif options['threads'] is None:
    options['threads'] = count // options['jobs']
  elif options['jobs'] is None:
    options['jobs'] = count // options['threads']

  # the number of streams defaults to the number of threads per job
  if options['streams'] is None:
    options['streams'] = options['threads']

  for config in opts.configs:
    process = parseProcess(config)

    if run_io_benchmark:
      # prepare a trimmed down configuration for benchmarking only reading the input data
      io_process = copy.deepcopy(process)
      io_process.hltGetRaw = cms.EDAnalyzer("HLTGetRaw", RawDataCollection = cms.InputTag("rawDataCollector"))
      io_process.path = cms.Path(io_process.hltGetRaw)
      io_process.schedule = cms.Schedule(io_process.path)
      if 'PrescaleService' in io_process.__dict__:
        del io_process.PrescaleService

      # benchmark reading the input data
      print('Benchmarking only I/O')
      io_options = dict(options, logdir = None, keep = [])
      multiCmsRun(io_process, **io_options)
      run_io_benchmark = False
      print()

    print('Benchmarking %s' % config)
    multiCmsRun(process, **options)
